# 習題 11 : 資訊理論 (Information Theory)

本專案實作了機率計算、熵的度量以及經典的漢明編碼，並探討夏農 (Shannon) 的重要定理。

## ⚠️ 來源宣告
> 本作業程式碼與數學原理解析由本人參考課堂講義與網路資源 (維基百科)，並在 AI (ChatGPT/Gemini) 協助下整理並撰寫程式碼驗證。

## 1. 機率下溢與對數 (Underflow & Logarithm)

### 問題
當我們計算連續 10000 次投擲硬幣皆為正面的機率 $P = 0.5^{10000}$ 時，數值約為 $10^{-3010}$。這遠遠小於計算機浮點數能表示的最小值 (Underflow)，導致結果直接變成 `0.0`。

### 解決方案
資訊理論使用 **對數機率 (Log Probability)** 來處理這種情況。
$$\log_2(p^n) = n \cdot \log_2(p)$$
計算結果為 $-10000$ (bits)，這代表該事件發生的「驚訝程度」極高，或者資訊量極大。

## 2. 熵與交叉熵 (Entropy & Cross Entropy)

### 定義
* **熵 (Entropy) $H(P)$**: 衡量一個機率分佈的不確定性。
    $$H(P) = -\sum P(x) \log_2 P(x)$$
* **交叉熵 (Cross Entropy) $H(P, Q)$**: 使用分佈 $Q$ 來編碼分佈 $P$ 所需的平均位元數。
    $$H(P, Q) = -\sum P(x) \log_2 Q(x)$$
* **KL 散度 (KL Divergence)**: 衡量兩個分佈的差異。
    $$D_{KL}(P||Q) = H(P, Q) - H(P)$$

### 題目勘誤與驗證
題目要求驗證 `cross_entropy(p,p) > cross_entropy(p,q)`，但根據 **吉布斯不等式 (Gibbs' Inequality)**，當 $Q \neq P$ 時，KL 散度恆大於 0，意即：
$$H(P, Q) > H(P, P)$$
**交叉熵在預測分佈 $Q$ 完全等於真實分佈 $P$ 時會有最小值。**
因此，程式碼中驗證的是正確的數學關係：$H(P, P) < H(P, Q)$。

## 3. 漢明碼 (Hamming Code 7-4)

### 原理
漢明碼是一種線性錯誤更正碼。對於 (7,4) 碼：
* **資料位元 (Data bits)**: 4 bits
* **校驗位元 (Parity bits)**: 3 bits (由資料位元的線性組合產生)
* **總長度**: 7 bits

### 錯誤更正機制
解碼時計算 **校驗子 (Syndrome)** $S = r \cdot H^T$。
* 若 $S = 0$，表示無錯誤。
* 若 $S \neq 0$，則 $S$ 的值指出哪一個 bit 發生了錯誤（可以更正 1 個 bit 的錯誤）。

## 4. 香農定理 (Shannon's Theorems)

### (1) 夏農信道編碼定理 (Noisy-Channel Coding Theorem)
又稱 **香農極限定理**。
它指出對於任何有雜訊的信道，只要傳輸速率 $R$ 小於信道容量 $C$，就存在一種編碼方式，可以使錯誤率任意趨近於 0。
* 意義：定義了通訊速度的理論上限，告訴我們在雜訊下也能實現可靠通訊。

### (2) 夏農-哈特利定理 (Shannon-Hartley Theorem)
這個定理給出了在高斯雜訊信道 (AWGN) 下的信道容量 $C$ 公式：
$$C = B \log_2 \left( 1 + \frac{S}{N} \right)$$
* $C$: 信道容量 (bit/s)
* $B$: 頻寬 (Hz)
* $S/N$: 信噪比 (Signal-to-Noise Ratio)

**意義**：要增加傳輸速度，必須增加頻寬 $B$ 或者增強訊號強度 (提高 $S/N$)。這也是為什麼 5G 比 4G 快的原因（頻寬更大、信噪比處理更好）。
