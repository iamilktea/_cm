# 習題 10 : 線性代數 (觀念與實作)

本作業包含兩部分：線性代數核心觀念的問答解析，以及使用 Python 實作行列式計算、矩陣分解 (LU, SVD) 與 PCA 演算法。

## Part 1: 觀念習題 (Conceptual Questions)

### 1. 線性代數中的『線性』與『代數』
* **線性 (Linear)**: 指的是運算符合「可加性 ($f(x+y)=f(x)+f(y)$)」與「齊次性 ($f(ax)=af(x)$)」。幾何上表現為直線、平面（不彎曲），代數上表現為一次方。
* **代數 (Algebra)**: 指的是研究符號結構、運算規則的學問。線性代數就是將線性關係符號化（如 $Ax=b$），讓我們能用代數方法操作幾何問題。

### 2. 『空間』與『向量空間』
* **空間 (Space)**: 在數學中，指一個集合加上定義在其上的結構（規則）。
* **向量空間 (Vector Space)**: 是一個集合，其元素（向量）可以進行「加法」與「純量乘法」，且運算結果仍在該集合內（封閉性），並符合結合律、分配律等八大公設。稱為「空間」是因為它提供了幾何操作的場所。

### 3. 矩陣與向量的關係
* **向量**: 是空間中的一個「點」或「方向」，是資料的載體。
* **矩陣**: 是將一個向量變換到另一個向量的「函數」或「映射」。
* **意義**: 矩陣代表了一種**線性變換**（如旋轉、縮放、剪切）。$Ax$ 的意義就是用 $A$ 這個變換規則去作用在 $x$ 上。

### 4. 矩陣如何代表 2D/3D 變換
* **縮放/旋轉**: 可以直接用 $2 \times 2$ (2D) 或 $3 \times 3$ (3D) 矩陣乘法達成。
* **平移 (Translation)**: 平移不是線性變換（原點會移動）。為了解決這個問題，我們引入**齊次座標 (Homogeneous Coordinates)**，將 $N$ 維點升級到 $N+1$ 維，用矩陣的最後一列來實現平移。

### 5. 行列式 (Determinant)
* **意義**: 矩陣變換後，「體積」的縮放比例。若為 0 代表維度塌縮（如 3D 壓扁成 2D，體積變 0）。
* **遞迴公式**: 利用拉普拉斯展開 (Laplace Expansion)，將大矩陣拆解為子矩陣行列式的加權和。
* **快速計算**:
    * **對角化**: $A = PDP^{-1}$，$\det(A) = \det(D) = \prod \lambda_i$（特徵值乘積）。
    * **LU 分解**: $A=LU$，$\det(A) = \det(L)\det(U) = 1 \times \prod U_{ii}$（上三角矩陣對角線乘積）。

### 6. 特徵值 (Eigenvalue) 與特徵向量 (Eigenvector)
* **意義**: 在矩陣變換下，**方向不改變**的向量稱為特徵向量；該方向上**長度伸縮的倍率**稱為特徵值 ($Av = \lambda v$)。
* **用途**: 特徵值分解可以將複雜的矩陣運算解耦 (Decoupling)，用於計算矩陣次方 ($A^n$)、微分方程求解、物理振動模態分析等。

### 7. QR 分解
* 將矩陣 $A$ 分解為正交矩陣 $Q$ 與上三角矩陣 $R$ 的乘積 ($A=QR$)。
* 它是 Gram-Schmidt 正交化過程的矩陣表示形式，常用於解最小平方法 (Least Squares) 問題。

### 8. 反覆 QR 分解求特徵值 (QR Algorithm)
* 這是一種數值演算法。透過反覆迭代：$A_k = Q_k R_k$ 然後令 $A_{k+1} = R_k Q_k$。
* $A_k$ 最終會收斂到一個上三角矩陣（Schur form），其對角線元素即為 $A$ 的特徵值。

### 9. SVD 分解 (Singular Value Decomposition)
* **是什麼**: 任何 $m \times n$ 矩陣都可以分解為 $A = U \Sigma V^T$。
    * $V$: 輸入空間的旋轉 (正交基底)。
    * $\Sigma$: 座標軸方向的縮放 (奇異值)。
    * $U$: 輸出空間的旋轉。
* **與特徵值關係**: SVD 是特徵值分解的推廣。$A^T A$ 的特徵值開根號即為 $A$ 的奇異值。SVD 適用於任何形狀的矩陣，而特徵值分解僅限方陣。

### 10. PCA 主成份分析
* **是什麼**: 一種降維技術，透過找尋資料變異量 (Variance) 最大的方向，將高維資料投影到低維空間。
* **與 SVD 關係**: PCA 的核心步驟是計算「共變異數矩陣」的特徵向量。這等價於對將資料中心化後的矩陣 $X$ 做 SVD 分解，其右奇異向量 $V$ 即為主成份方向。

---

## Part 2: 程式實作說明 (`linear_algebra.py`)

本程式使用 `numpy` 實作了題目要求的五個功能：

1.  **遞迴行列式 (`det_recursive`)**:
    * 實作拉普拉斯展開法，透過遞迴呼叫 `get_minor` 計算子矩陣。
    * *註：此方法時間複雜度為 $O(n!)$，僅適合教學演示，實際應用應使用 LU 分解。*

2.  **LU 分解 (`lu_decomposition_manual`)**:
    * 實作 Doolittle 演算法將 $A$ 分解為下三角 $L$ 與上三角 $U$。
    * 利用性質 $\det(A) = \det(L) \times \det(U) = 1 \times \prod U_{ii}$ 快速計算行列式。

3.  **分解驗證 (`verify_decompositions`)**:
    * 使用 `scipy.linalg.lu` 與 `numpy.linalg` 的標準函數，驗證 $A$ 是否等於分解後的矩陣相乘，確認分解的正確性。

4.  **SVD 實作 (`svd_via_eigen`)**:
    * 不直接呼叫 `svd`，而是透過計算 $A^T A$ 的特徵值與特徵向量來求得 $V$ 與 $\Sigma$。
    * 再透過 $u_i = \frac{1}{\sigma_i} A v_i$ 計算左奇異向量 $U$。

5.  **PCA 實作 (`pca_implementation`)**:
    * 步驟：資料中心化 $\to$ 計算共變異數矩陣 $\to$ 特徵值分解 $\to$ 選取最大特徵值對應的向量 $\to$ 投影。
    * 成功將 3D 資料降維投影。

### 執行結果摘要
```text
1. 遞迴行列式結果: -306.0
2. LU 分解行列式結果: -306.00
[LU] A == P*L*U? True
[Eigen] A*V == V*D? True
[SVD] A == U*S*Vh? True
手算 SVD Singular Values: [4.47213595 2.23606798]
